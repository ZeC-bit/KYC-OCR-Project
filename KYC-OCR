#Please Refer to the Read.me page in this link.
https://github.com/ZeC-bit/KYC-OCR-Project/blob/main/README.md
#Question 1. KYC and AML
!pip install opencv-python
!pip install matplotlib
!pip install numpy
!pip install -U pip jsoncomparison
!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
!pip install easyocr
#1. Reading image files from Google Drive
from google.colab import drive
drive.mount('/gdrive', force_remount=True)
from IPython.display import Image
Image('/gdrive/My Drive/kyc/hkidSample.png') ## HKID Information
Image('/gdrive/My Drive/kyc/informationSample.jpg') ## Personal Information of the Customer
import cv2
import numpy as np
import pandas as pd
import json
import os
import easyocr
import urllib
import io
import matplotlib.pyplot as plt
%matplotlib inline
im_1_path = '/gdrive/My Drive/kyc/hkidSample.png'
im_2_path = '/gdrive/My Drive/kyc/informationSample.jpg'

def recognize_text(img_path):
    '''loads an image and recognizes text.'''

    reader = easyocr.Reader(['en'])
    return reader.readtext(img_path)
result = recognize_text(im_1_path)
result
img_1 = cv2.imread(im_1_path)
img_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)
plt.imshow(img_1)
img_2 = cv2.imread(im_2_path)
img_2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB)
plt.imshow(img_2)
def overlay_ocr_text(img_path, save_name):
    '''loads an image, recognizes text, and overlays the text on the image.'''

    # loads image
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    dpi = 80
    fig_width, fig_height = int(img.shape[0]/dpi), int(img.shape[1]/dpi)
    plt.figure()
    f, axarr = plt.subplots(1,2, figsize=(fig_width, fig_height))
    axarr[0].imshow(img)

    # recognize text
    result = recognize_text(img_path)

    # if OCR prob is over 0.5, overlay bounding box and text
    for (bbox, text, prob) in result:
        if prob >= 0.5:
            # display
            print(f'Detected text: {text} (Probability: {prob:.2f})')

            # get top-left and bottom-right bbox vertices
            (top_left, top_right, bottom_right, bottom_left) = bbox
            top_left = (int(top_left[0]), int(top_left[1]))
            bottom_right = (int(bottom_right[0]), int(bottom_right[1]))

            # create a rectangle for bbox display
            cv2.rectangle(img=img, pt1=top_left, pt2=bottom_right, color=(255, 0, 0), thickness=10)

            # put recognized text
            cv2.putText(img=img, text=text, org=(top_left[0], top_left[1] - 10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=8)

    # show and save image
    axarr[1].imshow(img)
    plt.savefig(f'./output/{save_name}_overlay.jpg', bbox_inches='tight')
#3. Draw Bounding Boxes in Multiple Lines
img = cv2.imread(im_1_path)
# Image size with DPI
plt.figure(figsize=(20, 20), dpi=100)
i = 0
for detection in result:
  # Top left coordinate
    top_left = tuple([int (val) for val in detection[0][0]])
  # Bottom right coordinate
    bottom_right = tuple([int (val) for val in detection[0][2]])
  # Text extraction
    text = detection[1]
    print(str(i)+' '+text)
  # Draw Rectangle
    img = cv2.rectangle(img,top_left,bottom_right,(0,0,0),2)
    i+=1

plt.imshow(img)
plt.savefig('Detected_Text.png')
plt.show()
text = [result[res][1] for res in range(len(result))]
text
result = recognize_text(im_2_path)
img = cv2.imread(im_2_path)
# Image size with DPI
plt.figure(figsize=(20, 20), dpi=100)
i = 0
for detection in result:
  # Top left coordinate
    top_left = tuple([int (val) for val in detection[0][0]])
  # Bottom right coordinate
    bottom_right = tuple([int (val) for val in detection[0][2]])
  # Text extraction
    text = detection[1]
    print(str(i)+' '+text)
  # Draw Rectangle
    img = cv2.rectangle(img,top_left,bottom_right,(0,0,0),2)
    i+=1

plt.imshow(img)
plt.savefig('Detected_Text.png')
plt.show()
text = [result[res][1] for res in range(len(result))]
text
# 4. Prediction Score Histogram
data = []
for i in range(len(result)):
  # Append the prediction score in the list
  data.append([result[i][2]])
  # Create the dataframe of the prediction score
  df = pd.DataFrame(data,columns=["Prediction_Score"])

display(df.head())
df.plot.hist(bins=20)
plt.savefig("Prediction_Score.png", dpi=100)
# Statistical Summary of score
df.describe()
#5. Write Output Text in the File
textfile = open("Output.txt", "w")
# Write output text in the file
for i in range(len(result)):
  textfile.write(str(i) +': ' + str([result[i][1]])+ "\n")

textfile.close()
#6.Extracting and storing Detected Bounding Boxes
# Create Directory to store patches
os.mkdir ('/content/Patches')
%cd '/content/Patches'
for i in range(len(result)):
  X= int(result[i][0][0][0])            # //Column
  Y= int(result[i][0][0][1])            # //Row
  W= int(result[i][0][1][0])            # //Width
  H= int(result[i][0][2][1])            # //Height
# Slicing of particular boxes
  cropped_image = img[Y:Y+H, X:X+W]
# Save an image
  cv2.imwrite(str(i)+'.png', cropped_image)
# Zip the patches Folder
# /Destination /Source
!zip -r /content/Patches.zip /content/Patches
# Download the patches folder
from google.colab import files
files.download("/content/Patches.zip")
#7. Converting output text in Json format

import json

# Assuming you have extracted text data from images and stored it in a variable called `extracted_text`
extracted_text = """0: ['LANDSCAPE']
1: ['SERVICES']
2: ['Landscape Services']
3: ['#300 10130']
4: ['03 Sireet Norhiwest']
5: ['Edrionton; Alberta T5J 3N9']
6: ['888-721-1115']
7: ['nellb@serviceprovider_Diz']
8: ['WW/A.serviceprovider biz']
9: ['AECIPIENT:']
10: ['Invoice #1051']
11: ['Casey Young']
12: ['Issued']
13: ['Nol sent yel']
14: ['289 Norlnwesi 198ih Streel']
15: ['Jan 28, 2022']
16: ['Shoreline, Washinglon 98177']
17: ['PO#']
18: ['1.0 M']
19: ['Total']
20: ['32,058.00']
21: ['Ceccot']
22: ['12,L50 C']
23: ['For Services Rendered']
24: ['product']
25: ['service']
26: ['deScriPtION']
27: ['QTY:']
28: ['UnI']
29: ['TOTAL']
30: ['price']
31: ['Labcur']
32: ['Cne Iull day $ labor Ior =']
33: ['lecnnicians &nd']
34: ['575.00']
35: ['Sed0.00']
36: ['equipmenl 575 per nour:']
37: ['Startiped Corcrete']
38: ['Slamped concrele palh running lenglh l yard']
39: ['8510.00']
40: ['8510.00']
41: ['Slonpwctk']
42: ['Sione paln rurning lerigth ol yard_']
43: ['$475.00']
44: ['3475.00']
45: ['Brick']
46: ['Brick walkway running length ol yard.']
47: ['8375.00']
48: ['5375.00']
49: ['Sublolal']
50: ['51,960.00']
51: ['Landscane Senices Falds Oir cstomers Make thelr dream:']
52: ['cmarnue']
53: ['Corgeou3 /303']
54: ['O1 Misgipn I5 10 provice exkglte [andscapina :E ices']
55: ['GST (5.0%)']
56: ['598.00']
57: ["Meri' Edmonton homes"]
58: ['pos3ible']
59: ['Tolal']
60: ['32,058.00']
61: ['Knank"\'ouior curcubine3s']
62: ['Please contec: Us witn 24} quesbons regerding this']
63: ['MYcce']
64: ['AccounL palance']
65: ['82,058.00']
66: ['Jaakute']
"""

# Create a dictionary to store the extracted text
extracted_data = {
    "text": extracted_text
}

# Convert the dictionary to JSON format
json_data = json.dumps(extracted_data)

# Print the JSON data
print(json_data)
json_data
# Verify the required information
if "Chiu Fung Shek" in extracted_text and ("November 6th 1965" in extracted_text or "11-06-1965" in extracted_text):
    if "M" in extracted_text or "Gentleman" in extracted_text:
        if "09-08-03" in extracted_text or "September 8th 2003" in extracted_text:
            print("He is the correct person.")
        else:
            print("The date of issue is missing.")
    else:
        print("He is not the right person.")
else:
    print("He is not the right person.")
#8. Image Preprocessing Using Open CV
im_1_path = '/gdrive/My Drive/kyc/hkidSample.png'
im_2_path = '/gdrive/My Drive/kyc/informationSample.jpg'
src = cv2.imread('/gdrive/My Drive/kyc/hkidSample.png', 1)
## Noise Removal / Black
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
gray = cv2.GaussianBlur(gray, (3, 3), 0)
plt.imshow(gray)

# Canny Edge Finding
canned = cv2.Canny(gray, 150, 300)
plt.imshow(canned)

# Connection Edges
kernel = np.ones((10,1),np.uint8) # 가로 1 세로 10
mask = cv2.dilate(canned, kernel, iterations = 20)
plt.imshow(mask)
# Tilted Image
src = cv2.imread('/gdrive/My Drive/kyc/hkidSampleTilted.png', 1)
plt.imshow(src)
# Find out the contours
contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

# Finding Biggest contours
biggest_cntr = None
biggest_area = 0
for contour in contours:
    area = cv2.contourArea(contour)
    if area > biggest_area:
        biggest_area = area
        biggest_cntr = contour

# Outline box
rect = cv2.minAreaRect(biggest_cntr)
box = cv2.boxPoints(rect)
box = np.int0(box)

# Angle Calculation
angle = rect[-1]
if angle > 45:
    angle = -(75 - angle)

# Regulating Tilt
rotated = src.copy()
(h, w) = rotated.shape[:2]
center = (w // 2, h // 2)
M = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated = cv2.warpAffine(rotated, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

plt.imshow(rotated)

ones = np.ones(shape=(len(box), 1))
points_ones = np.hstack([box, ones])
transformed_box = M.dot(points_ones.T).T

y = [transformed_box[0][1], transformed_box[1][1], transformed_box[2][1], transformed_box[3][1]]
x = [transformed_box[0][0], transformed_box[1][0], transformed_box[2][0], transformed_box[3][0]]

y1, y2 = int(min(y)), int(max(y))
x1, x2 = int(min(x)), int(max(x))

# crop
crop = rotated[y1:y2, x1:x2]


plt.imshow(crop)
#Thank you indeed for your evaluation!
